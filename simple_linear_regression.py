# -*- coding: utf-8 -*-
"""Simple Linear Regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_90pVuxgIo8sEk4D_1xvT6TJasbMmgTG

#### This file demonstrates the various methods of tensorflow and keras 
#### using Simple Linear Regression
 
 Reference : https://www.tensorflow.org/tutorials/keras/regression
"""

#importing relevant libraries 

# Use seaborn for pairplot
!pip install -q seaborn

# Use some functions from tensorflow_docs
!pip install -q git+https://github.com/tensorflow/docs

import pathlib
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

print(tf.__version__)

#for graphical representation 
import tensorflow_docs as tfdocs
import tensorflow_docs.plots
import tensorflow_docs.modeling

# getting the data 

dataset_path = keras.utils.get_file("auto-mpg.data", "http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data")
dataset_path

#importing using pandas
column_names = ['MPG','Cylinders','Displacement','Horsepower','Weight',
                'Acceleration', 'Model Year', 'Origin']
dataset = pd.read_csv(dataset_path, names=column_names,
                      na_values = "?", comment='\t',
                      sep=" ", skipinitialspace=True)
dataset.head()

"""##### Cleaning the data"""

# checking out the null values 
dataset.isna().sum()

#dropping out the null values
dataset = dataset.dropna()
dataset

dataset.info()

#converting the Origin column into one hot encoding
dataset['Origin'] = dataset['Origin'].map({1:'USA',2:'Europe',3:'Japan'})
dataset = pd.get_dummies(dataset,prefix='', prefix_sep='')

dataset.tail()

"""##### Splitting the data into train and test"""

X_train = dataset.sample(frac=0.8,random_state=0)
X_test = dataset.drop(X_train.index)

#checking out the relationship between the variables 
sns.pairplot(X_train[["MPG", "Cylinders", "Displacement", "Weight"]], diag_kind="kde")

# looking at how variables are correlated
plt.figure(figsize=(10,9)) 
sns.heatmap(X_train.corr(),annot=True,cbar=True)

# looking at the overall statistics
trainData_stats = X_train.describe()

trainData_stats.pop('MPG')

trainData_stats = trainData_stats.transpose()

# splitting the test dataset
y_train = X_train.pop('MPG')
y_test = X_test.pop('MPG')

"""##### Normalizing the dataset before training"""

def normalize(x):
  return (x - trainData_stats['mean'])/trainData_stats['std']

X_train_normalized = normalize(X_train)
X_test_normalized = normalize(X_test)

"""#### Building the model"""

def build_model():
  model = keras.Sequential([
                            layers.Dense(64,activation='relu',input_shape=[len(X_train.keys())]),
                            layers.Dense(64,activation='relu'),
                            layers.Dense(1)
                          ])
  
  optimizer = tf.keras.optimizers.RMSprop(0.001)

  model.compile(loss = 'mse',
                optimizer= optimizer,
                metrics = ['mae' ,'mse'])
  return model

model = build_model()
model.summary()

"""#### Plotting the model"""

from keras.utils.vis_utils import plot_model
plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)

"""#### Training the model"""

example_batch = X_train_normalized[:10]
example_result = model.predict(example_batch)
example_result

EPOCHS = 1000

history = model.fit(X_train_normalized,y_train,
                    epochs=EPOCHS,validation_split=0.2,
                    callbacks=[tfdocs.modeling.EpochDots()],
                    verbose=1)

#Visualize the model's training progress using the stats stored in the history object
hist = pd.DataFrame(history.history)
hist['epoch'] = history.epoch
hist.tail()

"""#### Plotting the learning curves"""

plotter = tfdocs.plots.HistoryPlotter(smoothing_std=2)
plotter.plot({'Basic':history},metric='mae')
plt.ylim(0,10)
plt.ylabel('MAE[MPG]')

plotter.plot({'Basic': history}, metric = "mse")
plt.ylim([0, 20])
plt.ylabel('MSE [MPG^2]')

"""#### Early Stopping"""

model = build_model()

early_stop = keras.callbacks.EarlyStopping(monitor='val_loss',patience=10)

early_history = model.fit(X_train_normalized,y_train,
                    epochs=EPOCHS,validation_split=0.2,
                    callbacks=[early_stop,tfdocs.modeling.EpochDots()],verbose=1)

#plotting the learning curves 
plotter.plot({'Early Stopping': early_history}, metric = "mae")
plt.ylim([0, 10])
plt.ylabel('MAE [MPG]')

# checking the generalization of the model 
loss, mae, mse = model.evaluate(X_test_normalized, y_test, verbose=2)

print("Testing set Mean Abs Error: {:5.2f} MPG".format(mae))

"""#### Making Predictions"""

predictions = model.predict(X_test_normalized).flatten()
predictions

## Error distribution
error = predictions - y_test
plt.hist(error, bins = 25)
plt.xlabel("Prediction Error [MPG]")
_ = plt.ylabel("Count")